{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a6926d-9333-4557-8663-03a4adc2122b",
   "metadata": {},
   "source": [
    "In this notebook, we will fine tune the transfer learning approach on VGG16. \n",
    "\n",
    "So far, our accuracy is capped at 89-90%. We will follow the following steps to re-run the model on the entire dataset.\n",
    "\n",
    "- We will use __F1 score__ as the metric for evaluation instead of accuracy since the dataset is unbalanced. We will define a custom function for the F1 score\n",
    "- We will also have a separate test set in addition to the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ec214c-446d-4cd9-9935-4eee2ecb520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and functions\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from os import listdir\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Flatten, Dropout, BatchNormalization, Conv2D, MaxPool2D, Conv2DTranspose, Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1491fb9-1ba8-4231-a572-9d42c3559ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of AI generated images:  17856\n",
      "No of Real images:  3781\n"
     ]
    }
   ],
   "source": [
    "class0_folder= 'dataset/fakeV2/fake-v2'\n",
    "class1_folder= 'dataset/real'\n",
    "\n",
    "print(\"No of AI generated images: \",len(os.listdir(class0_folder)))\n",
    "print(\"No of Real images: \",len(os.listdir(class1_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ac2229-9e6c-4a5e-9e42-3a2363c9159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing .DS_Store: cannot identify image file 'dataset/fakeV2/fake-v2/.DS_Store'\n",
      "Error processing 12479.jpg: Image size (232748750 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.\n",
      "Error processing r-art.txt: cannot identify image file 'dataset/real/r-art.txt'\n"
     ]
    }
   ],
   "source": [
    "# Define a function to filter out excessively large images\n",
    "def filter_large_images(image_dir, max_pixels):\n",
    "    filtered_images = []\n",
    "    for filename in os.listdir(image_dir):\n",
    "        filepath = os.path.join(image_dir, filename)\n",
    "        try:\n",
    "            with Image.open(filepath) as img:\n",
    "                if img.size[0] * img.size[1] <= max_pixels:\n",
    "                    continue\n",
    "                    #filtered_images.append(filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            filtered_images.append(filename)\n",
    "    return filtered_images\n",
    "\n",
    "# Define your image directory and maximum allowable pixels\n",
    "\n",
    "max_pixels = 178956970  #The model was giving error for pixel sizes above this value; so testing with this as threshold\n",
    "\n",
    "# Filter out excessively large images from fake dataset\n",
    "filtered_images_fake = filter_large_images(class0_folder, max_pixels)\n",
    "\n",
    "# Filter out excessively large images from real dataset\n",
    "filtered_images_real = filter_large_images(class1_folder, max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9c27e4-6aa5-4c3e-afde-273f7e3aa072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load filenames and labels\n",
    "def load_filenames_labels(folder, label, large_img, sampled_imgs=None):\n",
    "    if sampled_imgs is None:\n",
    "        sampled_imgs= os.listdir(folder)\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if (filename not in large_img) and (filename in sampled_imgs) :\n",
    "            filenames.append(os.path.join(folder, filename))\n",
    "            labels.append(label)\n",
    "    return filenames, labels\n",
    "\n",
    "# Load filenames and labels for Class 0\n",
    "class0_filenames, class0_labels = load_filenames_labels(class0_folder, '0', filtered_images_fake)\n",
    "\n",
    "# Load filenames and labels for Class 1\n",
    "class1_filenames, class1_labels = load_filenames_labels(class1_folder, '1', filtered_images_real )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15806a89-e5bd-4d65-ae82-109f9da9e058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21634, 21634)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate filenames and labels from both classes\n",
    "all_filenames = class0_filenames + class1_filenames\n",
    "all_labels = class0_labels + class1_labels\n",
    "len(all_filenames), len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec26692-7cf6-4198-80fe-1210c9550afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15576, 15576)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test sets while maintaining class ratio; we will keep the test side aside\n",
    "full_train_filenames, test_filenames, full_train_labels, test_labels = train_test_split(\n",
    "    all_filenames, all_labels, test_size=0.1, stratify=all_labels, random_state=42)\n",
    "\n",
    "train_filenames, validation_filenames, train_labels, validation_labels = train_test_split(\n",
    "    full_train_filenames, full_train_labels, test_size=0.2, stratify=full_train_labels, random_state=42)\n",
    "\n",
    "len(train_filenames), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36bb1251-c6d7-4617-9760-23566f032ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 23:30:50.420003: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-04-07 23:30:50.420060: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-04-07 23:30:50.420067: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-04-07 23:30:50.420137: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-07 23:30:50.420168: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained VGG16 model with ImageNet weights\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Create ImageDataGenerator instances for train and validation sets along with the preprocessing for VGG16\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preprocess_input)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb66eeac-4704-4ca9-9853-b383b8bca90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15576 validated image filenames belonging to 2 classes.\n",
      "Found 3894 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#create the generators for train and validation data\n",
    "target_size= (224,224)\n",
    "batch_size= 64\n",
    "\n",
    "# Create the generator for training data\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({'filename': train_filenames, 'class': train_labels}),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create the generator for validation data\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({'filename': validation_filenames, 'class': validation_labels}),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d00fed39-d779-4b34-8356-0588cc361fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 256)               6422784   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " prediction (Dense)          (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21137986 (80.64 MB)\n",
      "Trainable params: 21137986 (80.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating model with pre trained imagenet weights\n",
    "base_model = VGG16(weights='imagenet')\n",
    "# creating our own model\n",
    "name= 'VGG16_partially_trained_full_data'\n",
    "x1 = Dense(256, activation='relu', name='fc1', kernel_regularizer=regularizers.l2(0.01))(base_model.layers[-4].output)\n",
    "x2= Dropout(0.5,name= 'dropout')(x1)\n",
    "y = Dense(2, activation='softmax', name='prediction')(x2)\n",
    "model = Model(base_model.input, y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc732c-8939-4672-ae2c-518950555c69",
   "metadata": {},
   "source": [
    "We will define a custom tf function for precision and recall and use this as the criteria/metric while compiling our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a1de612-4af1-4c93-a58c-7758f1165c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 256)               6422784   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " prediction (Dense)          (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21137986 (80.64 MB)\n",
      "Trainable params: 13502722 (51.51 MB)\n",
      "Non-trainable params: 7635264 (29.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create Precision and Recall metrics\n",
    "precision_metric = Precision()\n",
    "recall_metric = Recall()\n",
    "\n",
    "# Define F1-score metric function\n",
    "@tf.function\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_metric\n",
    "    recall = recall_metric\n",
    "    precision.update_state(y_true, y_pred)\n",
    "    recall.update_state(y_true, y_pred)\n",
    "    precision_result = precision.result()\n",
    "    recall_result = recall.result()\n",
    "    return 2 * ((precision_result * recall_result) / (precision_result + recall_result + 1e-10))\n",
    "    \n",
    "# to set the first 15 layers to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False \n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=[f1_score])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6432c2f-fa13-4bd9-ae0f-b0f399e89c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath= os.path.join(\"models\",f\"{name}.hdf5\")\n",
    "checkpoint= ModelCheckpoint(filepath=filepath, monitor=\"f1_score\",verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71cc1766-f837-4b7e-9b94-f08136a5b1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 22:30:54.064730: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - ETA: 0s - loss: 1.9915 - f1_score: 0.8454\n",
      "Epoch 1: f1_score improved from -inf to 0.84540, saving model to models/VGG16_partially_trained_full_data.hdf5\n",
      "244/244 [==============================] - 380s 2s/step - loss: 1.9915 - f1_score: 0.8454 - val_loss: 0.6957 - val_f1_score: 0.8874\n",
      "Epoch 2/5\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.4630 - f1_score: 0.9010\n",
      "Epoch 2: f1_score improved from 0.84540 to 0.90098, saving model to models/VGG16_partially_trained_full_data.hdf5\n",
      "244/244 [==============================] - 381s 2s/step - loss: 0.4630 - f1_score: 0.9010 - val_loss: 0.3777 - val_f1_score: 0.9107\n",
      "Epoch 3/5\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.2570 - f1_score: 0.9188\n",
      "Epoch 3: f1_score improved from 0.90098 to 0.91877, saving model to models/VGG16_partially_trained_full_data.hdf5\n",
      "244/244 [==============================] - 381s 2s/step - loss: 0.2570 - f1_score: 0.9188 - val_loss: 0.3014 - val_f1_score: 0.9242\n",
      "Epoch 4/5\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.1637 - f1_score: 0.9298\n",
      "Epoch 4: f1_score improved from 0.91877 to 0.92977, saving model to models/VGG16_partially_trained_full_data.hdf5\n",
      "244/244 [==============================] - 386s 2s/step - loss: 0.1637 - f1_score: 0.9298 - val_loss: 0.2664 - val_f1_score: 0.9341\n",
      "Epoch 5/5\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.1122 - f1_score: 0.9387\n",
      "Epoch 5: f1_score improved from 0.92977 to 0.93868, saving model to models/VGG16_partially_trained_full_data.hdf5\n",
      "244/244 [==============================] - 384s 2s/step - loss: 0.1122 - f1_score: 0.9387 - val_loss: 0.2723 - val_f1_score: 0.9421\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data and validate on the validation data\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=len(train_generator),\n",
    "                    epochs=5,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=len(validation_generator),\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "087c4583-9e41-4f35-9729-e2521d764cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 23:31:17.189855: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 74s 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      3214\n",
      "           1       0.78      0.79      0.78       680\n",
      "\n",
      "    accuracy                           0.92      3894\n",
      "   macro avg       0.87      0.87      0.87      3894\n",
      "weighted avg       0.92      0.92      0.92      3894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath=filepath)\n",
    "predictions= model.predict(validation_generator)\n",
    "\n",
    "predicted_labels =  np.argmax(predictions, axis=1).astype('str')\n",
    "true_labels= validation_labels\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd453a-8350-4a43-9b6b-b282a5fd5b94",
   "metadata": {},
   "source": [
    "### Testing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f43691fe-96f4-4d99-b775-107218c7c96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2164 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255, preprocessing_function=preprocess_input)\n",
    "# Create the generator for training data\n",
    "test_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({'filename': test_filenames, 'class': test_labels}),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d0fbfd6-056b-4f4d-81b1-adc24f705c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 41s 1s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      1786\n",
      "           1       0.79      0.77      0.78       378\n",
      "\n",
      "    accuracy                           0.92      2164\n",
      "   macro avg       0.87      0.86      0.87      2164\n",
      "weighted avg       0.92      0.92      0.92      2164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions= model.predict(test_generator)\n",
    "\n",
    "predicted_labels =  np.argmax(predictions, axis=1).astype('str')\n",
    "true_labels= test_labels\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc0688-478c-45f3-8039-5f861af88360",
   "metadata": {},
   "source": [
    "We have a macro-averaged f1-score of __87%__ and accuracy of __92%__. There seem to still be certain challenges to identify the real images. Since stable diffusion creates images from a noise signal, the noisy real art images maybe creating issues for the model in identifying them. An alternative approach to further improve the model would be to use a de-noising autoencoder in the pipeline prior to sending to VGG16 for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236321f8-963f-4701-ba36-8fb4564847be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
